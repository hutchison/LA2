\section{Eigenwerte und Eigenvektoren}

Sei $V$ ein $n$-dimensionaler Vektorraum über dem Körper $\K$.

%---------------------------------------------Definition 1.3------------------------------------------------
%----------------------------------------Eigenwert, Eigenvektor---------------------------------------------

\begin{mydef} Eigenwert und Eigenvektor \par
    Sei $\alpha$ ein Endomorphismus des $\K$-VR V und $\lambda\in \K$. $\lambda$ heißt genau dann Eigenwert von $\alpha$, wenn ein Vektor $v\in 
    V\backslash\lbrace0\rbrace$ existiert mit
    \begin{equation*}
        \alpha(v) = \lambda v
    \end{equation*}
    Der Vektor $v$ heißt dann Eigenvektor zum Eigenwert $\lambda$.
\end{mydef}

%-----------------------------------------------Beispiel---------------------------------------------------

%\begin{bsp} \qquad \par
\textit{Beispiel:}
    Gegeben Sei $V=\R^2$ und der Endomorphismus $\alpha:(x,y)\mapsto (x+y,x+y)$. \par
    $\qquad \Rightarrow \alpha(x,y)=(x+y,x+y) = \lambda(x,y)=(\lambda x,\lambda y)$ \par
    $\begin{array}{ccccrccc}
        \qquad\qquad &\lambda=0 & \Rightarrow & y= & -x & \Rightarrow & v= &\langle \mata1\\-1\mate \rangle \\
        \qquad\qquad &\lambda=2 & \Rightarrow & y= &  x & \Rightarrow & v= &\langle \mata1\\ 1\mate \rangle
    \end{array}$ \par
    Bezüglich der Standardbasis des $\R^2$ hat $\alpha$ die Abbilungsmatrix $A=\mata1&1\\1&1\mate$, bzgl. der Basis $\langle(1,1),(1,-1)\rangle$ die 
    Abbildungsmatrix $D=\mata2&0\\0&0\mate$. Dann nennt man $A$ ähnlich zu $D$, denn $A'=T^{-1}AT$, wobei $T$ die Matrix des Basiswechsels ist.
%\end{bsp}

%----------------------------------------------------------------------------------------------------------
%-----------------------------------------------Satz 1.2---------------------------------------------------
%----------------------------------------------------------------------------------------------------------
\begin{mysatz} \qquad \par
    Sei $\alpha$ ein Endomorphismus von $V$, dann sind folgende Aussagen äqiuvalent
    \begin{itemize}
        \item [1.] $\lambda$ ist Eigenwert von $\alpha$
        \item [2.] $\det(\lambda \mbox{ id}_v-\alpha)=0$
        \item [3.] rg$(\lambda \mbox{ id}_v-\alpha)<n$
        \item [4.] $\ker(\lambda \mbox{ id}_v-\alpha) \neq \lbrace 0 \rbrace$
    \end{itemize}
\end{mysatz}


%---------------------------------------------Definition 1.3------------------------------------------------
%----------------------------------------charakteristisches Polynom-----------------------------------------
\begin{mydef}\label{charpol} \textit{charakteristisches Polynom} \par
    \begin{itemize}
        \item[ 1.]Sei $A$ eine $n\times n$-Matrix. Das charakteristische Polynom von $A$ (charpol $A$) ist das Polynom 
            \begin{equation*}
                \det(\lambda\cdot E-A)\in \K[\lambda].
            \end{equation*}
        \item[ 2.]Sei $\alpha$ ein Endomorphismus von $V$. Das charakteristische Polynom von $\alpha$ ist charpol $\alpha$=charpol $A$, wobei $A$ die 
            Darstellungsmatrix von $\alpha$ bzgl. einer Basis von $V$ ist.
    \end{itemize}
\end{mydef}

%-----------------------------------------------Bemerkung---------------------------------------------------
%\begin{bem}\qquad \par
\textit{Bemerkung:}
    \begin{enumerate}
        \item Die Eigenwerte von $\alpha$ sind die Nullstellen des charakteristischen Polynoms.
        \item Ähnliche Matrizen haben dasselbe charpol
            \begin{eqnarray*}
                \mbox{charpol }A' &=& \det(\lambda E-T^{-1}AT) = \det(T^{-1}\lambda ET-T^{-1}AT)=\det(T^{-1}(\lambda E-A)T)\\
                &=& \det(T^{-1}) \cdot \det(\lambda E-A) \cdot \det(T) = \det(\lambda E-A)\\
                &=& \mbox{charpol }A
            \end{eqnarray*}
    \end{enumerate}
%\end{bem}

%---------------------------------------------Definition 1.4------------------------------------------------
%------------------------------------------------Eigenraum--------------------------------------------------
\begin{mydef}\label{eigenraum} \textit{Eigenraum} \par
    Sei $\alpha:V\to V$ ein Endomorphismus und $\lambda \in \K$ ein Eigenwert von $\alpha$, dann heißt
    \begin{equation*}
        E(\lambda) = \lbrace v\in V | \alpha(v)=\lambda v \rbrace
    \end{equation*}
    der Eigenraum zum Eigenwert $\lambda$.
\end{mydef}

%-----------------------------------------------Bemerkung---------------------------------------------------
%\begin{bem} \qquad \par
\textit{Bemerkung:}
    \begin{enumerate}
        \item $E(\lambda)$ ist die Menge aller EV von $\lambda$ und $0$
        \item $E(\lambda) = \ker(\lambda\mbox{ id}_v-\alpha) )$
        \item $E(\lambda)\leq V$
    \end{enumerate}
%\end{bem}


%---------------------------------------------Definition 1.5------------------------------------------------
%------------------------------------Diagonalmatrix, diagonalisierbar---------------------------------------
\begin{mydef} \textit{Diagonalmatrix, diagonalisierbar} \par
    Eine $n\times n$-Matrix $A$ heißt Diagonalmatrix, wenn $a_{ij}=0$ für $i\neq j, \ i,j=1,\ldots,n$ gilt. \par \medskip
    Man nennt die Matrix $A$ genau dann diagonalisierbar, wenn $A$ zu einer Diagonalmatrix ähnlich ist. \par \medskip
    Ein Endomorphismus $\alpha$ heißt genau dann diagonalisierbar, wenn eine Basis $B$ von $V$ existiert, bzgl. der die Abbildungsmatrix eine 
    Diagonalmatrix ist.
\end{mydef}

%-----------------------------------------------Bemerkung---------------------------------------------------

%\begin{bem} \qquad \par
\textit{Bemerkung:}
    Der Endomorphismus $\alpha$ ist genau dann diagonalisierbar, wenn es eine Basis von $V$ gibt, welche aus Eigenvektoren von $\alpha$ besteht. Die 
    verschiedenen Einträge auf der Diagonalen sind dann genau di Eigenwerte von $\alpha$.
%\end{bem}

%-----------------------------------------------Beispiel---------------------------------------------------
%\begin{bsp} \qquad \par
\textit{Beispiel:}
    Gegeben sei $V=\R^3$ und ein Endomrophismus in $V$, welcher bzgl. der Standardbasis die folgende Abbildungsmatrix besitzt.
    \begin{equation*}
        A=\mata4&0&2\\-6&1&-4\\-6&0&-3\mate. \mbox{ Nach \ref{charpol} ist dann charpol }A=\det(\lambda E-A)=(\lambda-1)^2\cdot \lambda 
    \end{equation*}
    Demnach sind nach (TODO)%\ref{eigenraum}
    \begin{eqnarray*}
        E(\la)&=&\ker(\la E-A)=\ker\mata-3&0&-2\\6&0&4\\6&0&4\mate=\langle \mata0\\1\\0\mate,\mata-2\\0\\3\mate \rangle \\
        E(\lb)&=&\ker(\lb E-A)=\ker\mata-4&0&-2\\6&-1&4\\6&0&3\mate =\langle \mata1\\-2\\-2\mate \rangle
    \end{eqnarray*}
    Die Eigenräume zu den Eigenwerten $\la=1$ und $\lb=0$. Demnach ist die Matrix $A$ ähnlich zu der Matrix
    \begin{equation*}
        A'=\mata1&0&0\\0&1&0\\0&0&0\mate \qquad \mbox{ mit }\qquad A'=T^{-1}\cdot A\cdot T\qquad \mbox{ und }\qquad T=\mata0&-2&1\\1&0&-2\\0&3&-2\mate
    \end{equation*}
    In der Transformationsmatrix $T$ stehen spaltenweise die Eigenvektoren.

%\end{bsp}

%-----------------------------------------------Bemerkung---------------------------------------------------
%\begin{bem}\qquad \par
\textit{Bemerkung:}
    Die Summe der Eigenwerte einer Matrix (mit der Vielfachheit genommen) entspricht der Spur der Matrix. Sei $A\in \R^{n \times n}$ ähnlich zu der 
    Matrix $D=T^{-1}AT$ dann ist $D$ eine Diagonalmatrix und es gilt
    \begin{equation*}
        Sp D = Sp(T^{-1}(AT))=Sp(T^{-1}TA)=Sp(A)
    \end{equation*} 
%\end{bem}
%-----------------------------------------------Lemma 1.6---------------------------------------------------
%-----------------------------------------------------------------------------------------------------------
\begin{mylemma}\label{lemdiag} \qquad \par
    Seien $v_1,\ldots,v_s$ Eigenvektoren zu verschiedenen Eigenwerten $\lambda_1,\ldots,\lambda_s$ eines Endomorphimus $\alpha$ von $V$. Dann ist
    \begin{equation*}
        \sum_{i=1}^s v_i \neq 0
    \end{equation*}

\textit{Beweis:}
%\begin{bew}
    Induktion über $s$. Klar für $s=1$, da per Definition $0$ kein Eigenvektor ist. Sei $s>1$ und $w_i=(\lambda_s-\lambda_i)\cdot v_i$ für $i=1,\ldots,s-1$. Dann ist $w_i$ ebenfalls ein Eigenvektor zum Eigenwert $\lambda_i$, denn
    \begin{equation*}
        \alpha(w_i)=\alpha((\lambda_s-\lambda_i)\cdot v_i)=(\lambda_s-\lambda_i)\cdot \alpha(v_i)= \lambda_i\cdot (\lambda_s-\lambda_i)\cdot v_i
    \end{equation*}
    Nun folgt für den Induktionsschritt
    \begin{equation*}
        (\lambda_s\mbox{ id}_v - \alpha)\cdot \sum_{i=1}^s v_i = \sum_{i=1}^s (\lambda_s\mbox{ id}_v - \alpha)\cdot v_i
        =\sum_{i=1}^{s-1} (\lambda_s-\lambda_i)\cdot v_i = \sum_{i=1}^{s-1} w_i \neq 0
    \end{equation*}
%\end{bew}
\end{mylemma}


%----------------------------------------------------------------------------------------------------------
%-----------------------------------------------Satz 1.7---------------------------------------------------
%----------------------------------------------------------------------------------------------------------
\begin{mysatz} \qquad \par
    Sei $\alpha$ ein Endomorphismus von $V$ und $\lambda_1,\ldots,\lambda_s$ verschiedene Eigenwerte von $\alpha$ in $\K$. Für $j=1,\ldots,s$ sei $
    d_j=\dim E(\lambda_j)$. Dann gilt:
    \begin{enumerate}
        \item $\displaystyle{\sum_{j=1}^s d_j \leq n}$
        \item $\displaystyle{\sum_{j=1}^s d_j = n \Leftrightarrow \alpha}$ ist diagonalisierbar
    \end{enumerate}

%\begin{bew} \par
\textit{Beweis:}
    \begin{enumerate}
        \item Sei für $j=1,\ldots,s$ $\lbrace b_{ji}|i=1,\ldots,d_j \rbrace$ eine Basis von $E(\lambda_j)$. Dann ist die Vereinigung aller Basen 
            $\lbrace b_{ji}|=1,\ldots,s; i=1,\ldots,d_j \rbrace$ linear unabhängig, denn wenn
            \begin{equation*}
                0=\sum_{j,i} k_{ji}b_{ji} = \sum_{j=1}^s v_j, \quad \mbox{wobei} \quad v_j=\sum_j k_{ji}b_{ji} 
            \end{equation*}
            und (für festes $j$) nicht alle $k_{ji}=0$, dann ist $v_j\neq0$, also ein Eigenvektor zu $\lambda_j$, was aber \ref{lemdiag} 
            widerspricht. Dann folgt auch schon die Behauptung, denn
            \begin{equation*}
                n\geq |\lbrace b_{ji}|=1,\ldots,s; i=1,\ldots,d_j \rbrace|=\sum_{j=1}^s d_j
            \end{equation*}
        \item In diesem Fall existiert eine Basis $\lbrace b_{ji}|=1,\ldots,s; i=1,\ldots,d_j \rbrace$, die nur Eigenvektoren enthält. Damit ist 
            $\alpha$ diagonalisierbar. Sei umgekehrt $\alpha$ diagonalisierbar. Dann ist $\lbrace b_{ji}|i=1,\ldots,d_j \rbrace$ eine Basis von 
            $E(\lambda_j)$. Damit folgt die Behauptung.
    \end{enumerate}
%\end{bew}
\end{mysatz}

%-----------------------------------------------Bemerkung---------------------------------------------------
%\begin{bem} \qquad \par
\textit{Bemerkung:}
    Sind die Eigenwerte eines Endomorphismus $\alpha$ alle verschieden, dann ist $\alpha$ diagonalisierbar. Die Umkehrung gilt im Allgemeinen nicht! 
    \par\medskip
    $\alpha$ ist diagonalisierbar, wenn das charpol vollständig in Linearfaktoren zerfällt und \par $\dim E(\lambda_i)=$ Vielfachheit der Nullstelle 
    $\lambda_i$ in charpol $\alpha$. \par \medskip
    Falls $\alpha$ diagonalisierbar ist, so ist $\displaystyle{V=\sum_{i=1}^s E(\lambda_i)}$\par
    $\qquad \Rightarrow E(\lambda_i) \cap E(\lambda_j) = \lbrace 0 \rbrace \quad i\neq j$
%\end{bem}

%---------------------------------------------Definition 1.8------------------------------------------------
%----------------------------------------direkte Summe, Projektion------------------------------------------
\begin{mydef} \textit{Direkte Summe, Projektion} \par
    Sei $V$ ein Vektorraum über $\K$ und $U_1,\ldots,U_s$ Unterräume von $V$. Man nennt $V$ die direkte Summe der $U_i$ genau dann, wenn jeder Vektor 
    $v$ aus $V$ sich eindeutig als Summe
    \begin{equation*}
        v=\sum_{i=1}^s u_i \qquad \mbox{ mit } u_i\in U_i
    \end{equation*}
    darstellen lässt. (Bez. $V=U_1\oplus\ldots\oplus U_s$)\par \medskip
    Wenn $\displaystyle{V=\sum_{i=1}^s \oplus U_i}$, dann sei $\pi_i:V\rightarrow U_i$ definiert durch
    \begin{equation*}
        \pi_i\left(\sum_{j=1}^s u_j\right) = u_i
    \end{equation*}
    Man nennt $\pi_i$ die Projektion von $V$ auf $U_i$.
\end{mydef}


%-----------------------------------------------Bemerkung---------------------------------------------------

%\begin{bem}\qquad \par
\textit{Bemerkung:}
    \begin{itemize}
        \item [1.]Der Durchschnitt zweier Unterräume einer direkten Summe ist stets trivial \par $\rightarrow U_i\cap U_j=\lbrace0\rbrace$ für $i\neq j$.
        \item [2.]$\displaystyle{\dim V=\sum_{i=1}^s\dim U_i}$.
        \item [3.]Eine Summe aus Unterräumen heiß direkt, wenn sich die Unterräume paarweise trivial schneiden.
        \item [4.]Die Sume von Eigenräumen zu verschiedenen Eigenwerten ist direkt.
        \item [5.]$\alpha$ ist genau dann diagonalisierbar, wenn $V$ die direkte Summe der Eigenräume $E(\lambda_i)$ zu den verschiedenen Eigenwerten 
            $\lambda_i$ ist.
        \item [6.]$\pi_i$ ist ein Epimorphismus (surektive lineare Abbildung).
    \end{itemize}
%\end{bem}

%----------------------------------------------------------------------------------------------------------
%-----------------------------------------------Satz 1.9---------------------------------------------------
%----------------------------------------------------------------------------------------------------------
\begin{mysatz} \qquad \par
    Seien $\alpha$ und $\beta$ diagonalisierbare Endomorphismen, welche miteinander kommutieren (d.h. $\alpha\beta=\beta\alpha$). Dann sind $\alpha$ und 
    $\beta$ gemeinsam diagonalisierbar, d.h. es gibt eine Basis aus Eigenvektoren von $\alpha$ und $\beta$

%\begin{bew}
\textit{Beweis:}
    $\alpha$ ist diagonalisierbar $\Rightarrow \lambda_1,\ldots,\lambda_s$ seien verschiedene Eigenwerte und $U_1,\ldots,U_s$ die dazugehörigen 
    Eigenräume mit $V=U_1\oplus\ldots\oplus U_s$. \par \medskip
    Sei $u_i\in U_i$. Dann ist auch $\beta(u_i)=u_i'\in U_i$, weil
    \begin{equation*}
        \alpha(u_i')= \alpha(\beta(u_i)) = \beta\alpha(u_i) = \beta \lambda_i u_i = \lambda_i u_i'.
    \end{equation*}
    Da $\beta$ diagonalisierbar ist, existiert eine Basis $\lbrace b_1,\ldots,b_n \rbrace$ von $V$ aus Eigenvektoren von $\beta$, d.h. 
    $\beta(b_j)=\mu_j\cdot b_j$. Jedes $b_j$ lässt sich eindeutig schreiben als $\displaystyle{b_j=\sum_{i=1}^s u_{ij}}$ mit $u_{ij}\in U_i.$ Dann ist
    \begin{equation*}
        \sum_{i=1}^s \beta(u_{ij}) = \beta(b_j)=\mu_j\cdot b_j = \sum_{i=1}^s\mu_j u_{ij}
    \end{equation*}
    Da $u_{ij}'=\beta(u_{ij})\in U_i$ für jeden $i$ und $\mu_ju_{ij}\in U_i$, folgt die Eindeutigkeit $\beta(u_{ij})=\mu_j u_{ij}$. Demnach ist, wenn 
    $u_{ij}\neq0$ gitl $u_{ij}$ ein Eigenwert zu $\alpha$ mit $\alpha(u_{ij})=\lambda_i\cdot u_{ij}$, aber auch ein Eigenwert zu $\beta$ mit 
    $\beta(u_{ij})=\mu_j\cdot u_{ij}$ und daher ist die Menge $\lbrace u_{ij}|i=1,\ldots,s;j=1,\ldots,n \rbrace$ ein Erzeugnendensystem, aus dem man 
    eine Basis wählen kann.
%\end{bew}
\end{mysatz}

%-----------------------------------------------Beispiel---------------------------------------------------
%\begin{bsp} \qquad \par
\textit{Beispiel:}
    Gegeben Sei $V=\R^2$ und die Endomorphismen $\alpha,\beta$, die bzgl. der Standardbasis die folgenen Abbildungsmatrizen besitzen.
    \begin{equation*}
        \alpha:A=\mata1&0\\1&2\mate \qquad \mbox{und} \qquad \beta:B=\mata3&0\\-4&-1\mate \qquad \mbox{mit} \qquad AB=BA=\mata3&0\\-5&-2\mate
    \end{equation*}
    Dann ist 
    \begin{equation*}
        E_{\alpha}(1) = \langle \mata1\\-1\mate \rangle = E_{\beta}(3) \mbox{ und }	E_{\alpha}(2) = \langle \mata0\\ 1\mate \rangle = E_{\beta}(-1)
    \end{equation*}
%\end{bsp}

%\newpage
